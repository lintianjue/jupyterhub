{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pandas** is a popular Python package for data science, and with good reason: it offers powerful, expressive and flexible data structures that make data manipulation and analysis easy, among many other things. \n",
    "\n",
    "# Leaning Objectives\n",
    "\n",
    "### In this module, we will cover:\n",
    "\n",
    "* How to create a Pandas DataFrame in a file format (.csv, .txt, .tsv, etc.)\n",
    "* Use pandas functions to select, add, delete an Index or Column from / to a DataFrame \n",
    "* How to do the various queries from DataFrame\n",
    "* Lastly, how various SQL operations would be performed using Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data files into DataFrame\n",
    "\n",
    "  --  **pandas.read_csv()**\n",
    "\n",
    "Thankfully, Pandas has built-in support for delimited files such as CSV files as well as a variety of other data formats including relational databases, Excel, and HTML tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the relevant CSV data from:\n",
    "                                             https://s3.amazonaws.com/amazon-reviews-pds/tsv/index.txt\n",
    "                                             \n",
    "**Amazon's sample public dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to import the pandas library to your notebook\n",
    "import pandas as pd\n",
    "\n",
    "# Read data from Amazon's sample public dataset.\n",
    "df = pd.read_csv(\"Datasets/sample_us.tsv\", sep='\\t', header=0)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: \n",
    " --  **pandas.head()**\n",
    "\n",
    "The head function is really helpful in just previewing what the dataframe looks like after you have loaded it up. The default is to show the first 5 rows, but you can adjust that by typing ```.head(10)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " --  **pandas.describe()**\n",
    "\n",
    "The describe function is really useful to see the distribution of your data, particularly numerical fields like ints and floats. As you can see below, it returns a dataframe with the mean, min, max, standard deviation, etc for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Querying a Series\n",
    "\n",
    "* To query by numerical location, stating at zero, use the **iloc** attribute\n",
    "* To query by the index label, you can use the **loc** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.loc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enough for now about selecting values from your DataFrame. What about selecting rows and columns? In that case, you would use:\n",
    "\n",
    "* To select rows and cloumns, use the **loc** attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.loc[:, ['marketplace', 'product_id']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['marketplace'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may either access the values by calling by their label or by their position in the index or column.\n",
    "\n",
    "```.loc[3]['marketplace']```  is equivalent to  ```.loc[3, 'marketplace']```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[3, 'marketplace']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Sort Pandas Dataframe\n",
    "\n",
    "* To sort Pandas DataFrame based on the values of a column or multiple columns, use the **.sort_values()** method.\n",
    "* To sort the values in ascending or descending orders, use the argument **ascending=False**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_by_rating = df.sort_values('star_rating')\n",
    "sort_by_rating.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sort_by_votes = df.sort_values('total_votes', ascending = False)\n",
    "sort_by_votes.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sort Pandas DataFrame based on the values of **multiple columns**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sort_by_rating_votes = df.sort_values(['star_rating', 'total_votes'], ascending = False)\n",
    "sort_by_rating_votes.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Querying with Conditions\n",
    "\n",
    "* To query the values **WHERE** certain conditions are met, via *boolean indexing*.\n",
    "\n",
    "**Comparison with SQL**\n",
    "\n",
    "```\n",
    "   SELECT *\n",
    "   FROM Table_1\n",
    "   WHERE star_rating = 4\n",
    "   LIMIT 5;\n",
    "```\n",
    "\n",
    "The following statement is simply passing a Series of True/False objects to the DataFrame, returning all rows with True.\n",
    "\n",
    "```\n",
    "   df[df['star_rating'] == 4].head(5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df['star_rating'] == 4].head(5)\n",
    "\n",
    "# Similarly, you may use df.loc[df['star_rating'] == 4].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Multiple** conditions:\n",
    "\n",
    "Just like SQL’s OR and AND, multiple conditions can be passed to a DataFrame using | (OR) and & (AND).\n",
    "\n",
    "**Comparison with SQL**\n",
    "\n",
    "```\n",
    "   SELECT *\n",
    "   FROM Table_1\n",
    "   WHERE star_rating = 5 AND total_votes > 3\n",
    "   LIMIT 5;\n",
    "```\n",
    "\n",
    "The following statement is simply passing a Series of True/False objects to the DataFrame, returning all rows with True.\n",
    "\n",
    "```\n",
    "   df[(df['star_rating'] == 5) & (df['total_votes'] > 3)].head(4)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[(df['star_rating'] == 5) & (df['total_votes'] >= 3)].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison with SQL**\n",
    "\n",
    "```\n",
    "   SELECT *\n",
    "   FROM Table_1\n",
    "   WHERE star_rating > 3 or total_votes > 3\n",
    "   LIMIT 5;\n",
    "```\n",
    "\n",
    "The following statement is simply passing a Series of True/False objects to the DataFrame, returning all rows with True.\n",
    "\n",
    "```\n",
    "   df[(df['star_rating'] > 3) | (df['total_votes'] > 3)].head(4)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df['star_rating'] > 3) | (df['total_votes'] > 3)].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NULL checking is done using the ```.notna()``` and ```.isna()``` methods.\n",
    "\n",
    "**For example**\n",
    "```\n",
    "    df[df['star_rating'].isna()]```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: GROUP BY conditions\n",
    "\n",
    "In pandas, SQL’s ```GROUP BY``` operations are performed using the similarly named ```.groupby()``` method. ```.groupby()``` typically refers to a process where we’d like to split a dataset into groups, apply some function (typically **aggregation**) , and then combine the groups together.\n",
    "\n",
    "A common SQL operation would be getting the count of records in each group throughout a dataset. For instance, a query getting us the number of tips left by sex:\n",
    "\n",
    "\n",
    "**Comparison with SQL**\n",
    "\n",
    "```\n",
    "   SELECT star_rating, count(*)\n",
    "   FROM Table_1\n",
    "   GROUP BY star_rating;\n",
    "```\n",
    "\n",
    "The equivalent pandas statement would be:\n",
    "\n",
    "```\n",
    "   df.groupby('star_rating').size()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('star_rating').size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the pandas code we used ```.size()``` and not ```.count()```. This is because ```count()``` applies the function to each column, returning the number of not null records within each.\n",
    "\n",
    "```\n",
    "  df.groupby('star_rating').count()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.groupby('star_rating').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Aggregate functions\n",
    "\n",
    "Multiple functions can also be applied at once. For instance, say we’d like to see how tip amount differs by day of the week - ```.agg()``` allows you to pass a dictionary to your grouped DataFrame, indicating which functions to apply to specific columns.\n",
    "\n",
    "Aggregating functions are the ones that reduce the dimension of the returned objects. Some common aggregating functions are tabulated below:\n",
    "\n",
    "Function | Description\n",
    "------------- | -------------\n",
    "mean() | Compute mean of groups\n",
    "sum() |\tCompute sum of group values\n",
    "size() | Compute group sizes\n",
    "count()\t| Compute count of group\n",
    "std()\t| Standard deviation of groups\n",
    "var()\t| Compute variance of groups\n",
    "sem()\t| Standard error of the mean of groups\n",
    "describe()\t| Generates descriptive statistics\n",
    "first()\t| Compute first of group values\n",
    "last()\t| Compute last of group values\n",
    "nth()\t| Take nth value, or a subset if n is a list\n",
    "min()\t| Compute min of group values\n",
    "max()\t| Compute max of group values\n",
    "\n",
    "**Comparison with SQL**\n",
    "\n",
    "```\n",
    "   SELECT star_rating, AVG(total_votes),count(*)\n",
    "   FROM Table_1\n",
    "   GROUP BY star_rating;\n",
    "```\n",
    "\n",
    "The equivalent pandas statement would be:\n",
    "\n",
    "```\n",
    "   df.groupby('star_rating').agg({'total_votes': np.mean, 'star_rating': np.size})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['star_rating'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['star_rating'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "df.groupby('star_rating').agg({'total_votes': [np.mean, np.size], 'star_rating': np.size})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Adding new column to existing DataFrame\n",
    "\n",
    "Let’s discuss how to add new columns to existing DataFrame in Pandas. There are multiple ways we can do this task.\n",
    "\n",
    "**Method #1:** By declaring a new list as a column.\n",
    "\n",
    "```\n",
    "   import numpy as np\n",
    "   npRandNumbers = np.random.rand(len(df))\n",
    "   default_val = 3.33\n",
    "   \n",
    "   df['rand_number'] = npRandNumbers\n",
    "   df['default_value'] = default_val\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "npRandNumbers = np.random.rand(len(df))\n",
    "default_val = 3.33\n",
    "   \n",
    "df['rand_number'] = npRandNumbers\n",
    "df['default_value'] = default_val\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Method #2:** Adding a calculated field.\n",
    "\n",
    "```\n",
    "   df['calculated_value'] = df['total_votes'] / df['star_rating']\n",
    "```\n",
    "OR use the **lambda** function via ```.assign()```\n",
    "```\n",
    "   df = df.assign(percentage_help_votes = lambda x: (x['helpful_votes']/x['star_rating'])*100)\n",
    "```\n",
    "OR use the ```.transform()``` methods\n",
    "```\n",
    "   df['avg_total_votes'] = df.groupby('star_rating')['total_votes'].transform('mean')\n",
    "   # may try 'sum', 'zscore'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['calculated_value'] = df['total_votes'] / df['star_rating']\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(percentage_help_votes = lambda x: (x['helpful_votes']/x['star_rating'])*100)\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['sum_total_votes'] = df.groupby('star_rating')['total_votes'].transform('sum')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method #3:** Adding columns via ```.concat()```.\n",
    "\n",
    "```   \n",
    "   df_company = pd.DataFrame(['Amazon'] * len(df), columns = ['company'])\n",
    "   pd.concat([df, df_company], axis = 1)\n",
    "```\n",
    "Combine DataFrame objects horizontally along the ```x``` axis by passing in ```axis=1```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_company = pd.DataFrame(['Amazon'] * len(df), columns = ['company'])\n",
    "pd.concat([df, df_company], axis = 1).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Append rows to existing DataFrame\n",
    "\n",
    "Iteratively appending rows to a DataFrame can be more computationally intensive than a single concatenate. A better solution is to append those rows to a list and then concatenate the list with the original DataFrame all at once.\n",
    "\n",
    "* To append another DataFrame to an existing DataFrame, via ```.append()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df\n",
    "df2['product_category'] = 'Furniture'\n",
    "\n",
    "df = df.append(df2, ignore_index = True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code: Drop column(s) from an existing DataFrame\n",
    "\n",
    "This function takes a single parameter, which is the index or roll label, to drop.\n",
    "\n",
    "* In fact, the **drop()** doesn't change the DataFrame by default\n",
    "* And instead, returns to you a copy of the DataFrame with the given rows removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop('rand_number', axis=1).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JOIN -- Table Merge\n",
    "\n",
    "For those with experience doing joins in relational databases like SQL, here’s some good news: pandas has options for high performance in-memory merging and joining. When we need to combine very large DataFrames, joins serve as a powerful way to perform these operations swiftly.\n",
    "\n",
    "A couple important things to keep in mind: joins can only be done on two DataFrames at a time, denoted as left and right tables. The key is the common column that the two DataFrames will be joined on. It’s a good practice to use keys which have unique values throughout the column to avoid unintended duplication of row values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the relevant CSV data from:\n",
    "```\n",
    "    Fixed_deposit.csv & Loan.csv\n",
    "```\n",
    "                                             \n",
    "**sample customer bank dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command to import the pandas library to your notebook\n",
    "import pandas as pd\n",
    "\n",
    "# Read data from Fixed_deposit.csv dataset.\n",
    "df_fixed_deposit = pd.read_csv(\"Datasets/Fixed_deposit.csv\", sep=',', header=0)\n",
    "df_fixed_deposit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data from Fixed_deposit.csv dataset.\n",
    "df_loan = pd.read_csv(\"Datasets/Loan.csv\", sep=',', header=0)\n",
    "df_loan.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **inner join** is the simplest join, this will only retain rows in which both tables share a key value.\n",
    "\n",
    "<img src=\"Datasets/inner_join.png\" alt=\"Drawing\" style=\"width: 250px;\"/>\n",
    "\n",
    "### Code: Inner Join\n",
    "\n",
    "* To do a inner join for two DataFrames, via ```.merge()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_bank_InnerJoin = pd.merge(df_fixed_deposit, df_loan, left_on = 'Customer ID', right_on = 'Customer ID', how = 'inner')\n",
    "df_customer_bank_InnerJoin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **left join** keeps all rows that occur in the primary (left) table, and the right table will only concatenate on rows where it shares a key value with the left. ```NaN``` values will be filled in for cells where the there’s no matching key value.\n",
    "\n",
    "<img src=\"Datasets/left_join.png\" alt=\"Drawing\" style=\"width: 250px;\"/>\n",
    "\n",
    "### Code: Left Join\n",
    "\n",
    "* To do a left join for two DataFrames, via ```.merge()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_bank_LeftJoin = pd.merge(df_fixed_deposit, df_loan, left_on = 'Customer ID', right_on = 'Customer ID', how = 'left')\n",
    "df_customer_bank_LeftJoin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **right join** is the same concept as a left join, but keeps all rows occurring in the right table. The resulting DataFrame will have any potential ```NaN``` values on the left side.\n",
    "\n",
    "<img src=\"Datasets/right_join.png\" alt=\"Drawing\" style=\"width: 250px;\"/>\n",
    "\n",
    "### Code: Right Join\n",
    "\n",
    "* To do a right join for two DataFrames, via ```.merge()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_bank_RightJoin = pd.merge(df_fixed_deposit, df_loan, left_on = 'Customer ID', right_on = 'Customer ID', how = 'right')\n",
    "df_customer_bank_RightJoin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An **full outer join** retains all rows occuring in both tables and NaN values can show up on either side of your resulting DataFrame.\n",
    "\n",
    "<img src=\"Datasets/full_outer_join.png\" alt=\"Drawing\" style=\"width: 250px;\"/>\n",
    "\n",
    "### Code: Full Outer Join\n",
    "\n",
    "* To do a full outer join for two DataFrames, via ```.merge()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer_bank_FullJoin = pd.merge(df_fixed_deposit, df_loan, left_on = 'Customer ID', right_on = 'Customer ID', how = 'outer')\n",
    "df_customer_bank_FullJoin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referrences\n",
    "\n",
    "Boolean masks are created by applying operators directly to the pandas series or DataFrame objects.\n",
    "\n",
    "<img src=\"Datasets/DataFrame_with_BooleanMask.JPG\" width=\"750\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
